
--- [START 2017-07-20 19:16:22] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 123
	file    = /home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/code/standard-7-1/train-forest-0.py
	out_dir = /home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/results/dense121-jpg-1

** dataset setting **
	(height,width) = (224, 224)
	in_channels    = 3
	train_dataset.split  = train-40479
	train_dataset.num    = 40479
	test_dataset.split   = train-5000
	test_dataset.num     = 5000
	batch_size           = 32
	train_loader.sampler = <torch.utils.data.sampler.RandomSampler object at 0x7f619e65cf98>
	test_loader.sampler  = <torch.utils.data.sampler.SequentialSampler object at 0x7f619e03bf60>

def augment(x, u=0.75):
    if random.random()<u:
        if random.random()>0.5:
            x = randomDistort1(x, distort_limit=0.35, shift_limit=0.25, u=1)
        else:
            x = randomDistort2(x, num_steps=10, distort_limit=0.2, u=1)
        x = randomShiftScaleRotate(x, shift_limit=0.0625, scale_limit=0.10, rotate_limit=45, u=1)

    x = randomFlip(x, u=0.5)
    x = randomTranspose(x, u=0.5)
    x = randomContrast(x, limit=0.2, u=0.5)
    #x = randomSaturation(x, limit=0.2, u=0.5),
    x = randomFilter(x, limit=0.5, u=0.2)
    return x


** net setting **
<class 'net.model.densenet.DenseNet'>

    def __init__(self, in_shape=(3,244,244), num_classes=1000, growth_rate=32, block_config=(6, 12, 24, 16),
                 num_init_features=64, bn_size=4, drop_rate=0):

        super(DenseNet, self).__init__()
        in_channels, height, width = in_shape

        # First convolution
        self.features = nn.Sequential(OrderedDict([
            ('conv0', nn.Conv2d(in_channels, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),
            ('norm0', nn.BatchNorm2d(num_init_features)),
            ('relu0', nn.ReLU(inplace=True)),
            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),
        ]))

        # Each denseblock
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,
                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = num_features // 2

        # Final batch norm
        self.features.add_module('norm5', nn.BatchNorm2d(num_features))

        # Linear layer
        self.fc = nn.Linear(num_features, num_classes)

    def forward(self, x):

        features = self.features(x)
        out = F.relu(features, inplace=True)
        out = F.dropout(out, p=0.20, training=self.training)
        out = F.adaptive_avg_pool2d(out,output_size=1)
        out = out.view(out.size(0), -1)
        out = self.fc(out)

        logit = out
        prob = F.sigmoid(logit)
        return logit,prob


--missing keys at load_valid():--
	 fc.weight
	 fc.bias
------------------------
** start training here! **
 optimizer=<torch.optim.sgd.SGD object at 0x7f619dc7e8d0>
 LR=Step Learning Rates
rates=['0.1000', '0.0100', '0.0050', '0.0010', '0.0001', '0.0000', '-1.0000']
steps=['0', '10', '25', '35', '40', '50', '51']

 epoch   iter   rate  |  smooth_loss   |  train_loss  (acc)  |  valid_loss  (acc)  | min
----------------------------------------------------------------------------------------
  1.0    1264    0.1000   |  0.1242  | 0.1012  0.8894 | 0.1415  0.8629  |  9.4 min 
  2.0    1264    0.1000   |  0.1288  | 0.0887  0.9372 | 0.1369  0.8802  |  9.4 min 
  3.0    1264    0.1000   |  0.1377  | 0.1374  0.8623 | 0.1342  0.8876  |  9.4 min 
  4.0    1264    0.1000   |  0.1278  | 0.0927  0.9244 | 0.1654  0.8398  |  9.4 min 
  5.0    1264    0.1000   |  0.1257  | 0.1364  0.8578 | 0.1213  0.8996  |  9.3 min 
  6.0    1264    0.1000   |  0.1348  | 0.1534  0.8695 | 0.1251  0.8810  |  9.3 min 
  7.0    1264    0.1000   |  0.1341  | 0.1142  0.8777 | 0.1261  0.8872  |  9.3 min 
  8.0    1264    0.1000   |  0.1348  | 0.1381  0.8767 | 0.1482  0.8711  |  9.3 min 
  9.0    1264    0.1000   |  0.1323  | 0.1157  0.8899 | 0.1173  0.9039  |  9.3 min 
 10.0    1264    0.1000   |  0.1214  | 0.1106  0.8802 | 0.1459  0.8641  |  9.3 min 
 11.0    1264    0.0100   |  0.1073  | 0.0995  0.9049 | 0.1004  0.9168  |  9.3 min 
 12.0    1264    0.0100   |  0.1030  | 0.1167  0.8834 | 0.0980  0.9189  |  9.3 min 
 13.0    1264    0.0100   |  0.1052  | 0.0930  0.9305 | 0.0984  0.9170  |  9.3 min 
 14.0    1264    0.0100   |  0.1164  | 0.1185  0.8625 | 0.0982  0.9131  |  9.3 min 
 15.0    1264    0.0100   |  0.1098  | 0.1003  0.9144 | 0.0963  0.9173  |  9.3 min 
 16.0    1264    0.0100   |  0.1054  | 0.0899  0.9361 | 0.0962  0.9177  |  9.3 min 
 17.0    1264    0.0100   |  0.1124  | 0.0785  0.9405 | 0.0960  0.9184  |  9.3 min 
 18.0    1264    0.0100   |  0.1067  | 0.0961  0.9430 | 0.0980  0.9189  |  9.3 min 
 19.0    1264    0.0100   |  0.1035  | 0.1520  0.8509 | 0.0989  0.9138  |  9.3 min 
 20.0    1264    0.0100   |  0.1095  | 0.1055  0.9288 | 0.0993  0.9160  |  9.3 min 
 21.0    1264    0.0100   |  0.1130  | 0.0993  0.9279 | 0.0984  0.9141  |  9.4 min 
 22.0    1264    0.0100   |  0.1076  | 0.0814  0.9332 | 0.0961  0.9190  |  9.3 min 
 23.0    1264    0.0100   |  0.1142  | 0.1356  0.8975 | 0.1019  0.9130  |  9.4 min 
 24.0    1264    0.0100   |  0.1091  | 0.0977  0.9011 | 0.1012  0.9063  |  9.3 min 
 25.0    1264    0.0100   |  0.1059  | 0.1015  0.9098 | 0.0971  0.9154  |  9.3 min 
 26.0    1264    0.0050   |  0.1065  | 0.1351  0.8999 | 0.0911  0.9241  |  9.3 min 
 27.0    1264    0.0050   |  0.1049  | 0.1377  0.8799 | 0.0914  0.9214  |  9.3 min 
 28.0    1264    0.0050   |  0.1024  | 0.1494  0.8661 | 0.0906  0.9256  |  9.3 min 
 29.0    1264    0.0050   |  0.1073  | 0.0992  0.9271 | 0.0913  0.9233  |  9.3 min 
 30.0    1264    0.0050   |  0.1113  | 0.0975  0.8884 | 0.0914  0.9246  |  9.3 min 
 31.0    1264    0.0050   |  0.0967  | 0.0576  0.9642 | 0.0921  0.9224  |  9.3 min 
 32.0    1264    0.0050   |  0.0987  | 0.0732  0.9537 | 0.0917  0.9230  |  9.3 min 
 33.0    1264    0.0050   |  0.1010  | 0.0838  0.9206 | 0.0908  0.9230  |  9.3 min 
 34.0    1264    0.0050   |  0.1003  | 0.1335  0.8752 | 0.0925  0.9204  |  9.3 min 
 35.0    1264    0.0050   |  0.0937  | 0.0681  0.9483 | 0.0925  0.9206  |  9.3 min 
 36.0    1264    0.0010   |  0.0988  | 0.1182  0.9148 | 0.0859  0.9270  |  9.3 min 
 37.0    1264    0.0010   |  0.0961  | 0.0839  0.9339 | 0.0864  0.9256  |  9.3 min 
 38.0    1264    0.0010   |  0.0989  | 0.0658  0.9505 | 0.0860  0.9297  |  9.3 min 
 39.0    1264    0.0010   |  0.0957  | 0.1125  0.8887 | 0.0846  0.9284  |  9.3 min 
 40.0    1264    0.0010   |  0.0935  | 0.1255  0.8861 | 0.0857  0.9271  |  9.3 min 
 41.0    1264    0.0001   |  0.0969  | 0.1064  0.9174 | 0.0844  0.9292  |  9.3 min 
 42.0    1264    0.0001   |  0.0852  | 0.0469  0.9705 | 0.0848  0.9297  |  9.3 min 
 43.0    1264    0.0001   |  0.0945  | 0.0704  0.9556 | 0.0842  0.9286  |  9.3 min 
 44.0    1264    0.0001   |  0.0939  | 0.0450  0.9660 | 0.0844  0.9300  |  9.3 min 
 45.0    1264    0.0001   |  0.0972  | 0.0582  0.9622 | 0.0841  0.9293  |  9.3 min 
 46.0    1264    0.0001   |  0.0984  | 0.1077  0.8823 | 0.0844  0.9298  |  9.3 min 
 47.0    1264    0.0001   |  0.0919  | 0.0911  0.9112 | 0.0838  0.9298  |  9.3 min 
 48.0    1264    0.0001   |  0.0972  | 0.1217  0.8993 | 0.0839  0.9293  |  9.3 min 
 49.0    1264    0.0001   |  0.0950  | 0.0637  0.9423 | 0.0841  0.9301  |  9.3 min 
 50.0    1264    0.0001   |  0.0922  | 0.0658  0.9532 | 0.0842  0.9292  |  9.3 min 
 51.0    1264    0.0000   |  0.0918  | 0.0854  0.9275 | 0.0844  0.9296  |  9.3 min 

/home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/results/dense121-jpg-1/snap/final.torch:
	all time to train=491.6 min
	test_loss=0.084427, test_acc=0.929556
