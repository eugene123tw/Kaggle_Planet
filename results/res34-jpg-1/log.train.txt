
--- [START 2017-07-20 00:07:38] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 123
	file    = /home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/code/standard-7-1/train-forest-0.py
	out_dir = /home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/results/res34-jpg-1

** dataset setting **
	(height,width) = (256, 256)
	in_channels    = 3
	train_dataset.split  = train-40479
	train_dataset.num    = 40479
	test_dataset.split   = train-5000
	test_dataset.num     = 5000
	batch_size           = 64
	train_loader.sampler = <torch.utils.data.sampler.RandomSampler object at 0x7f303510d8d0>
	test_loader.sampler  = <torch.utils.data.sampler.SequentialSampler object at 0x7f303510d940>

def augment(x, u=0.75):
    if random.random()<u:
        if random.random()>0.5:
            x = randomDistort1(x, distort_limit=0.35, shift_limit=0.25, u=1)
        else:
            x = randomDistort2(x, num_steps=10, distort_limit=0.2, u=1)
        x = randomShiftScaleRotate(x, shift_limit=0.0625, scale_limit=0.10, rotate_limit=45, u=1)

    x = randomFlip(x, u=0.5)
    x = randomTranspose(x, u=0.5)
    x = randomContrast(x, limit=0.2, u=0.5)
    #x = randomSaturation(x, limit=0.2, u=0.5),
    x = randomFilter(x, limit=0.5, u=0.2)
    return x


** net setting **
<class 'net.model.resnet.ResNet'>

    def __init__(self, block, layers, in_shape=(3,244,244), num_classes=1000):
        self.inplanes = 64

        super(ResNet, self).__init__()
        in_channels, height, width = in_shape

        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1  = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        #x = F.dropout(x8,p=0.5,training=self.training)
        #x = nn.AvgPool2d(kernel_size=7)
        #x = F.adaptive_avg_pool2d(x,output_size=1) + F.adaptive_max_pool2d(x,output_size=1)
        #x = F.adaptive_max_pool2d(x8,output_size=1)
        x = F.adaptive_avg_pool2d(x,output_size=1)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        logit = x
        prob  = F.sigmoid(logit)
        return logit, prob


--missing keys at load_valid():--
	 fc.weight
	 fc.bias
------------------------
** start training here! **
 optimizer=<torch.optim.sgd.SGD object at 0x7f303470e2e8>
 LR=Step Learning Rates
rates=['0.1000', '0.0100', '0.0050', '0.0010', '0.0001', '-1.0000']
steps=['0', '10', '25', '35', '40', '43']

 epoch   iter   rate  |  smooth_loss   |  train_loss  (acc)  |  valid_loss  (acc)  | min
----------------------------------------------------------------------------------------
  1.0     632    0.1000   |  0.1084  | 0.1156  0.8868 | 0.1510  0.8561  |  5.4 min 
  2.0     632    0.1000   |  0.1010  | 0.1053  0.9138 | 0.1193  0.8876  |  5.0 min 
  3.0     632    0.1000   |  0.1094  | 0.0897  0.9180 | 0.1141  0.8974  |  5.0 min 
  4.0     632    0.1000   |  0.1030  | 0.0940  0.8852 | 0.1118  0.9020  |  5.0 min 
  5.0     632    0.1000   |  0.1064  | 0.1099  0.8981 | 0.1360  0.8683  |  5.0 min 
  6.0     632    0.1000   |  0.1132  | 0.1474  0.8642 | 0.1085  0.9100  |  5.0 min 
  7.0     632    0.1000   |  0.1100  | 0.0743  0.9432 | 0.1227  0.8856  |  5.0 min 
  8.0     632    0.1000   |  0.1168  | 0.1336  0.8500 | 0.1238  0.8912  |  4.9 min 
  9.0     632    0.1000   |  0.1156  | 0.0942  0.9431 | 0.1769  0.8273  |  5.0 min 
 10.0     632    0.1000   |  0.1126  | 0.1262  0.8882 | 0.1199  0.8945  |  5.0 min 
 11.0     632    0.0100   |  0.0994  | 0.1301  0.8764 | 0.0891  0.9245  |  4.9 min 
 12.0     632    0.0100   |  0.1038  | 0.0910  0.9010 | 0.0883  0.9229  |  5.0 min 
 13.0     632    0.0100   |  0.1021  | 0.1069  0.9164 | 0.0873  0.9267  |  5.0 min 
 14.0     632    0.0100   |  0.0899  | 0.0991  0.9167 | 0.0856  0.9265  |  4.9 min 
 15.0     632    0.0100   |  0.0972  | 0.0745  0.9365 | 0.0858  0.9275  |  5.0 min 
 16.0     632    0.0100   |  0.0990  | 0.0574  0.9567 | 0.0848  0.9275  |  4.9 min 
 17.0     632    0.0100   |  0.0941  | 0.0978  0.9024 | 0.0867  0.9247  |  5.0 min 
 18.0     632    0.0100   |  0.0927  | 0.0852  0.9359 | 0.0851  0.9292  |  5.0 min 
 19.0     632    0.0100   |  0.0965  | 0.0901  0.9138 | 0.0844  0.9277  |  5.0 min 
 20.0     632    0.0100   |  0.0930  | 0.0819  0.9405 | 0.0854  0.9274  |  5.0 min 
 21.0     632    0.0100   |  0.0929  | 0.1056  0.9042 | 0.0854  0.9271  |  5.0 min 
 22.0     632    0.0100   |  0.0939  | 0.1074  0.9076 | 0.0861  0.9252  |  5.0 min 
 23.0     632    0.0100   |  0.0931  | 0.0875  0.9347 | 0.0861  0.9273  |  5.0 min 
 24.0     632    0.0100   |  0.0953  | 0.0724  0.9363 | 0.0852  0.9228  |  4.9 min 
 25.0     632    0.0100   |  0.0935  | 0.0702  0.9355 | 0.0842  0.9280  |  5.0 min 
 26.0     632    0.0050   |  0.0861  | 0.1084  0.9157 | 0.0816  0.9272  |  4.9 min 
 27.0     632    0.0050   |  0.0916  | 0.0871  0.9096 | 0.0808  0.9317  |  4.9 min 
 28.0     632    0.0050   |  0.0871  | 0.0973  0.9164 | 0.0802  0.9304  |  4.9 min 
 29.0     632    0.0050   |  0.0874  | 0.0890  0.9016 | 0.0802  0.9301  |  4.9 min 
 30.0     632    0.0050   |  0.0935  | 0.1195  0.8870 | 0.0805  0.9305  |  4.9 min 
 31.0     632    0.0050   |  0.0914  | 0.0931  0.9259 | 0.0795  0.9321  |  4.9 min 
 32.0     632    0.0050   |  0.0879  | 0.0842  0.9116 | 0.0805  0.9297  |  4.9 min 
 33.0     632    0.0050   |  0.0895  | 0.1040  0.9051 | 0.0811  0.9324  |  4.9 min 
 34.0     632    0.0050   |  0.0940  | 0.1182  0.8953 | 0.0859  0.9242  |  4.9 min 
 35.0     632    0.0050   |  0.0825  | 0.0881  0.9220 | 0.0799  0.9320  |  4.9 min 
 36.0     632    0.0010   |  0.0816  | 0.0767  0.9369 | 0.0755  0.9356  |  4.9 min 
 37.0     632    0.0010   |  0.0852  | 0.1047  0.9042 | 0.0750  0.9355  |  4.9 min 
 38.0     632    0.0010   |  0.0908  | 0.0955  0.9348 | 0.0745  0.9368  |  4.9 min 
 39.0     632    0.0010   |  0.0828  | 0.0878  0.9146 | 0.0745  0.9364  |  4.9 min 
 40.0     632    0.0010   |  0.0813  | 0.0759  0.9362 | 0.0743  0.9367  |  4.9 min 
 41.0     632    0.0001   |  0.0790  | 0.0819  0.9136 | 0.0736  0.9380  |  4.9 min 
 42.0     632    0.0001   |  0.0796  | 0.0911  0.9319 | 0.0735  0.9377  |  4.9 min 
 43.0     632    0.0001   |  0.0809  | 0.0629  0.9517 | 0.0734  0.9374  |  4.9 min 

/home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/results/res34-jpg-1/snap/final.torch:
	all time to train=221.3 min
	test_loss=0.073426, test_acc=0.937401
