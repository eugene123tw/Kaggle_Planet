
--- [START 2017-07-18 21:01:41] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 123
	file    = /home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/code/standard-7-1/train-forest-0.py
	out_dir = /home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/results/res34-jpg-0

** dataset setting **
	(height,width) = (256, 256)
	in_channels    = 3
	train_dataset.split  = train-35479
	train_dataset.num    = 35479
	test_dataset.split   = train-5000
	test_dataset.num     = 5000
	batch_size           = 64
	train_loader.sampler = <torch.utils.data.sampler.RandomSampler object at 0x7f50996cb358>
	test_loader.sampler  = <torch.utils.data.sampler.SequentialSampler object at 0x7f50996cb3c8>

def augment(x, u=0.75):
    if random.random()<u:
        if random.random()>0.5:
            x = randomDistort1(x, distort_limit=0.35, shift_limit=0.25, u=1)
        else:
            x = randomDistort2(x, num_steps=10, distort_limit=0.2, u=1)
        x = randomShiftScaleRotate(x, shift_limit=0.0625, scale_limit=0.10, rotate_limit=45, u=1)

    x = randomFlip(x, u=0.5)
    x = randomTranspose(x, u=0.5)
    x = randomContrast(x, limit=0.2, u=0.5)
    #x = randomSaturation(x, limit=0.2, u=0.5),
    x = randomFilter(x, limit=0.5, u=0.2)
    return x


** net setting **
<class 'net.model.resnet.ResNet'>

    def __init__(self, block, layers, in_shape=(3,244,244), num_classes=1000):
        self.inplanes = 64

        super(ResNet, self).__init__()
        in_channels, height, width = in_shape

        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1  = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        #x = F.dropout(x8,p=0.5,training=self.training)
        #x = nn.AvgPool2d(kernel_size=7)
        #x = F.adaptive_avg_pool2d(x,output_size=1) + F.adaptive_max_pool2d(x,output_size=1)
        #x = F.adaptive_max_pool2d(x8,output_size=1)
        x = F.adaptive_avg_pool2d(x,output_size=1)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        logit = x
        prob  = F.sigmoid(logit)
        return logit, prob


--missing keys at load_valid():--
	 fc.weight
	 fc.bias
------------------------
** start training here! **
 optimizer=<torch.optim.sgd.SGD object at 0x7f5098da4be0>
 LR=Step Learning Rates
rates=['0.1000', '0.0100', '0.0050', '0.0010', '0.0001', '-1.0000']
steps=['0', '10', '25', '35', '40', '43']

 epoch   iter   rate  |  smooth_loss   |  train_loss  (acc)  |  valid_loss  (acc)  | min
----------------------------------------------------------------------------------------
  1.0     554    0.1000   |  0.1163  | 0.0977  0.9127 | 0.1192  0.8793  |  4.7 min 
  2.0     554    0.1000   |  0.1077  | 0.1238  0.9077 | 0.1361  0.8649  |  4.4 min 
  3.0     554    0.1000   |  0.1109  | 0.1347  0.8534 | 0.1196  0.8793  |  4.3 min 
  4.0     554    0.1000   |  0.1089  | 0.0985  0.9199 | 0.1565  0.8383  |  4.3 min 
  5.0     554    0.1000   |  0.1135  | 0.1194  0.9001 | 0.1181  0.8923  |  4.3 min 
  6.0     554    0.1000   |  0.1064  | 0.1037  0.9083 | 0.1390  0.8668  |  4.4 min 
  7.0     554    0.1000   |  0.1191  | 0.1270  0.8661 | 0.1245  0.8905  |  4.4 min 
  8.0     554    0.1000   |  0.1128  | 0.1026  0.9079 | 0.1225  0.8791  |  4.4 min 
  9.0     554    0.1000   |  0.1209  | 0.1455  0.8788 | 0.1541  0.8491  |  4.3 min 
 10.0     554    0.1000   |  0.1146  | 0.1047  0.9045 | 0.1681  0.8538  |  4.4 min 
 11.0     554    0.0100   |  0.0995  | 0.0806  0.9414 | 0.0907  0.9215  |  4.4 min 
 12.0     554    0.0100   |  0.0965  | 0.0814  0.9223 | 0.0893  0.9231  |  4.3 min 
 13.0     554    0.0100   |  0.0933  | 0.0843  0.9263 | 0.0890  0.9230  |  4.3 min 
 14.0     554    0.0100   |  0.0953  | 0.0891  0.9293 | 0.0883  0.9248  |  4.3 min 
 15.0     554    0.0100   |  0.0903  | 0.0739  0.9393 | 0.0902  0.9226  |  4.3 min 
 16.0     554    0.0100   |  0.0940  | 0.1030  0.9278 | 0.0890  0.9234  |  4.3 min 
 17.0     554    0.0100   |  0.0889  | 0.0720  0.9440 | 0.0896  0.9217  |  4.3 min 
 18.0     554    0.0100   |  0.0950  | 0.0862  0.9179 | 0.0881  0.9237  |  4.3 min 
 19.0     554    0.0100   |  0.0993  | 0.1177  0.8977 | 0.0890  0.9206  |  4.3 min 
 20.0     554    0.0100   |  0.0971  | 0.1007  0.8917 | 0.0891  0.9223  |  4.4 min 
 21.0     554    0.0100   |  0.0959  | 0.0959  0.9264 | 0.0896  0.9231  |  4.3 min 
 22.0     554    0.0100   |  0.0936  | 0.0970  0.9164 | 0.0889  0.9211  |  4.3 min 
 23.0     554    0.0100   |  0.0895  | 0.1083  0.9016 | 0.0887  0.9241  |  4.3 min 
 24.0     554    0.0100   |  0.0920  | 0.0802  0.9134 | 0.0905  0.9218  |  4.3 min 
 25.0     554    0.0100   |  0.0883  | 0.0893  0.9369 | 0.0889  0.9234  |  4.4 min 
 26.0     554    0.0050   |  0.0948  | 0.1239  0.8735 | 0.0870  0.9233  |  4.3 min 
 27.0     554    0.0050   |  0.0886  | 0.0726  0.9404 | 0.0865  0.9238  |  4.3 min 
 28.0     554    0.0050   |  0.0850  | 0.0722  0.9421 | 0.0877  0.9236  |  4.3 min 
 29.0     554    0.0050   |  0.0887  | 0.0739  0.9202 | 0.0854  0.9274  |  4.4 min 
 30.0     554    0.0050   |  0.0862  | 0.0958  0.9275 | 0.0871  0.9247  |  4.3 min 
 31.0     554    0.0050   |  0.0928  | 0.0894  0.9169 | 0.0856  0.9254  |  4.3 min 
 32.0     554    0.0050   |  0.0862  | 0.0844  0.9286 | 0.0857  0.9265  |  4.3 min 
 33.0     554    0.0050   |  0.0831  | 0.0646  0.9492 | 0.0875  0.9222  |  4.3 min 
 34.0     554    0.0050   |  0.0864  | 0.1284  0.8836 | 0.0861  0.9251  |  4.3 min 
 35.0     554    0.0050   |  0.0929  | 0.0976  0.9186 | 0.0859  0.9250  |  4.4 min 
 36.0     554    0.0010   |  0.0788  | 0.0599  0.9525 | 0.0836  0.9273  |  4.3 min 
 37.0     554    0.0010   |  0.0864  | 0.0770  0.9440 | 0.0835  0.9283  |  4.3 min 
 38.0     554    0.0010   |  0.0826  | 0.0812  0.9326 | 0.0843  0.9263  |  4.3 min 
 39.0     554    0.0010   |  0.0835  | 0.0776  0.9303 | 0.0844  0.9276  |  4.3 min 
 40.0     554    0.0010   |  0.0845  | 0.0925  0.9175 | 0.0844  0.9254  |  4.3 min 
 41.0     554    0.0001   |  0.0822  | 0.0971  0.9095 | 0.0840  0.9277  |  4.3 min 
 42.0     554    0.0001   |  0.0801  | 0.0798  0.9130 | 0.0838  0.9272  |  4.3 min 
 43.0     554    0.0001   |  0.0823  | 0.0943  0.9173 | 0.0837  0.9269  |  4.3 min 

/home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/results/res34-jpg-0/snap/final.torch:
	all time to train=195.3 min
	test_loss=0.083708, test_acc=0.926911
