
--- [START 2017-07-19 13:31:31] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 123
	file    = /home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/code/standard-7-1/train-forest-0.py
	out_dir = /home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/results/dense121-jpg-0

** dataset setting **
	(height,width) = (128, 128)
	in_channels    = 3
	train_dataset.split  = train-35479
	train_dataset.num    = 35479
	test_dataset.split   = train-5000
	test_dataset.num     = 5000
	batch_size           = 96
	train_loader.sampler = <torch.utils.data.sampler.RandomSampler object at 0x7f0eea199860>
	test_loader.sampler  = <torch.utils.data.sampler.SequentialSampler object at 0x7f0eea1997b8>

def augment(x, u=0.75):
    if random.random()<u:
        if random.random()>0.5:
            x = randomDistort1(x, distort_limit=0.35, shift_limit=0.25, u=1)
        else:
            x = randomDistort2(x, num_steps=10, distort_limit=0.2, u=1)
        x = randomShiftScaleRotate(x, shift_limit=0.0625, scale_limit=0.10, rotate_limit=45, u=1)

    x = randomFlip(x, u=0.5)
    x = randomTranspose(x, u=0.5)
    x = randomContrast(x, limit=0.2, u=0.5)
    #x = randomSaturation(x, limit=0.2, u=0.5),
    x = randomFilter(x, limit=0.5, u=0.2)
    return x


** net setting **
<class 'net.model.resnet.ResNet'>

    def __init__(self, block, layers, in_shape=(3,244,244), num_classes=1000):
        self.inplanes = 64

        super(ResNet, self).__init__()
        in_channels, height, width = in_shape

        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1  = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        #x = F.dropout(x8,p=0.5,training=self.training)
        #x = nn.AvgPool2d(kernel_size=7)
        #x = F.adaptive_avg_pool2d(x,output_size=1) + F.adaptive_max_pool2d(x,output_size=1)
        #x = F.adaptive_max_pool2d(x8,output_size=1)
        x = F.adaptive_avg_pool2d(x,output_size=1)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        logit = x
        prob  = F.sigmoid(logit)
        return logit, prob


--missing keys at load_valid():--
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 bn1.running_mean
	 bn1.running_var
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.bn1.running_mean
	 layer1.0.bn1.running_var
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.bn2.running_mean
	 layer1.0.bn2.running_var
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.bn1.running_mean
	 layer1.1.bn1.running_var
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.bn2.running_mean
	 layer1.1.bn2.running_var
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.bn1.running_mean
	 layer1.2.bn1.running_var
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.bn2.running_mean
	 layer1.2.bn2.running_var
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.bn1.running_mean
	 layer2.0.bn1.running_var
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.bn2.running_mean
	 layer2.0.bn2.running_var
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.0.downsample.1.running_mean
	 layer2.0.downsample.1.running_var
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.bn1.running_mean
	 layer2.1.bn1.running_var
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.bn2.running_mean
	 layer2.1.bn2.running_var
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.bn1.running_mean
	 layer2.2.bn1.running_var
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.bn2.running_mean
	 layer2.2.bn2.running_var
	 layer2.3.conv1.weight
	 layer2.3.bn1.weight
	 layer2.3.bn1.bias
	 layer2.3.bn1.running_mean
	 layer2.3.bn1.running_var
	 layer2.3.conv2.weight
	 layer2.3.bn2.weight
	 layer2.3.bn2.bias
	 layer2.3.bn2.running_mean
	 layer2.3.bn2.running_var
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.bn1.running_mean
	 layer3.0.bn1.running_var
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.bn2.running_mean
	 layer3.0.bn2.running_var
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.0.downsample.1.running_mean
	 layer3.0.downsample.1.running_var
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.bn1.running_mean
	 layer3.1.bn1.running_var
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.bn2.running_mean
	 layer3.1.bn2.running_var
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.bn1.running_mean
	 layer3.2.bn1.running_var
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.bn2.running_mean
	 layer3.2.bn2.running_var
	 layer3.3.conv1.weight
	 layer3.3.bn1.weight
	 layer3.3.bn1.bias
	 layer3.3.bn1.running_mean
	 layer3.3.bn1.running_var
	 layer3.3.conv2.weight
	 layer3.3.bn2.weight
	 layer3.3.bn2.bias
	 layer3.3.bn2.running_mean
	 layer3.3.bn2.running_var
	 layer3.4.conv1.weight
	 layer3.4.bn1.weight
	 layer3.4.bn1.bias
	 layer3.4.bn1.running_mean
	 layer3.4.bn1.running_var
	 layer3.4.conv2.weight
	 layer3.4.bn2.weight
	 layer3.4.bn2.bias
	 layer3.4.bn2.running_mean
	 layer3.4.bn2.running_var
	 layer3.5.conv1.weight
	 layer3.5.bn1.weight
	 layer3.5.bn1.bias
	 layer3.5.bn1.running_mean
	 layer3.5.bn1.running_var
	 layer3.5.conv2.weight
	 layer3.5.bn2.weight
	 layer3.5.bn2.bias
	 layer3.5.bn2.running_mean
	 layer3.5.bn2.running_var
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.bn1.running_mean
	 layer4.0.bn1.running_var
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.bn2.running_mean
	 layer4.0.bn2.running_var
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.0.downsample.1.running_mean
	 layer4.0.downsample.1.running_var
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.bn1.running_mean
	 layer4.1.bn1.running_var
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.bn2.running_mean
	 layer4.1.bn2.running_var
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.bn1.running_mean
	 layer4.2.bn1.running_var
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.bn2.running_mean
	 layer4.2.bn2.running_var
	 fc.weight
	 fc.bias
------------------------
** start training here! **
 optimizer=<torch.optim.sgd.SGD object at 0x7f0ee9887128>
 LR=Step Learning Rates
rates=['0.1000', '0.0100', '0.0050', '0.0010', '0.0001', '-1.0000']
steps=['0', '10', '25', '35', '40', '43']

 epoch   iter   rate  |  smooth_loss   |  train_loss  (acc)  |  valid_loss  (acc)  | min
----------------------------------------------------------------------------------------
  1.0     369    0.1000   |  0.1598  | 0.1621  0.8345 | 0.3437  0.7121  |  5.2 min 
  2.0     369    0.1000   |  0.1475  | 0.1514  0.8700 | 0.1649  0.8390  |  1.3 min 
  3.0     369    0.1000   |  0.1412  | 0.1365  0.8812 | 0.1492  0.8584  |  1.3 min 
  4.0     369    0.1000   |  0.1345  | 0.1199  0.8816 | 0.1822  0.8205  |  1.3 min 
  5.0     369    0.1000   |  0.1297  | 0.1036  0.9064 | 0.3887  0.6990  |  1.3 min 
  6.0     369    0.1000   |  0.1268  | 0.1433  0.8547 | 0.1356  0.8779  |  1.3 min 
  7.0     369    0.1000   |  0.1246  | 0.1267  0.8545 | 0.1481  0.8546  |  1.3 min 
  8.0     369    0.1000   |  0.1327  | 0.1597  0.8554 | 0.1439  0.8623  |  1.3 min 
  9.0     369    0.1000   |  0.1256  | 0.1318  0.8629 | 0.1678  0.8355  |  1.3 min 
 10.0     369    0.1000   |  0.1264  | 0.1490  0.8726 | 0.1733  0.8247  |  1.3 min 
 11.0     369    0.0100   |  0.1106  | 0.1044  0.9171 | 0.1140  0.9037  |  1.3 min 
 12.0     369    0.0100   |  0.1140  | 0.1004  0.9263 | 0.1120  0.9002  |  1.3 min 
 13.0     369    0.0100   |  0.1067  | 0.1034  0.8958 | 0.1083  0.9028  |  1.3 min 
 14.0     369    0.0100   |  0.1047  | 0.1024  0.9129 | 0.1044  0.9094  |  1.3 min 
 15.0     369    0.0100   |  0.1057  | 0.1032  0.9148 | 0.1001  0.9139  |  1.3 min 
 16.0     369    0.0100   |  0.1013  | 0.0940  0.9168 | 0.1016  0.9130  |  1.3 min 
 17.0     369    0.0100   |  0.1027  | 0.0732  0.9353 | 0.1020  0.9133  |  1.3 min 
 18.0     369    0.0100   |  0.1054  | 0.1103  0.9065 | 0.1101  0.9030  |  1.3 min 
 19.0     369    0.0100   |  0.1060  | 0.0954  0.9183 | 0.0998  0.9139  |  1.3 min 
 20.0     369    0.0100   |  0.1044  | 0.1162  0.9025 | 0.0992  0.9149  |  1.3 min 
 21.0     369    0.0100   |  0.1056  | 0.1032  0.9141 | 0.1065  0.9080  |  1.3 min 
 22.0     369    0.0100   |  0.1010  | 0.0748  0.9318 | 0.1440  0.8627  |  1.3 min 
 23.0     369    0.0100   |  0.1076  | 0.1027  0.9156 | 0.0987  0.9158  |  1.3 min 
 24.0     369    0.0100   |  0.0974  | 0.1069  0.9037 | 0.1184  0.9000  |  1.2 min 
 25.0     369    0.0100   |  0.1029  | 0.0988  0.9029 | 0.0983  0.9137  |  1.2 min 
 26.0     369    0.0050   |  0.0998  | 0.1106  0.9109 | 0.1058  0.9033  |  1.2 min 
 27.0     369    0.0050   |  0.1014  | 0.0946  0.9149 | 0.1023  0.9113  |  1.2 min 
 28.0     369    0.0050   |  0.1007  | 0.1134  0.8771 | 0.0970  0.9139  |  1.2 min 
 29.0     369    0.0050   |  0.1039  | 0.0879  0.9348 | 0.1424  0.8696  |  1.2 min 
 30.0     369    0.0050   |  0.1004  | 0.1048  0.9047 | 0.0952  0.9155  |  1.3 min 
 31.0     369    0.0050   |  0.0993  | 0.0895  0.9056 | 0.0965  0.9158  |  1.3 min 
 32.0     369    0.0050   |  0.0990  | 0.1048  0.9084 | 0.1011  0.9086  |  1.3 min 
 33.0     369    0.0050   |  0.0964  | 0.1030  0.9203 | 0.0969  0.9131  |  1.3 min 
 34.0     369    0.0050   |  0.1000  | 0.1127  0.8815 | 0.0985  0.9136  |  1.3 min 
 35.0     369    0.0050   |  0.0994  | 0.0999  0.8959 | 0.0975  0.9151  |  1.3 min 
 36.0     369    0.0010   |  0.0948  | 0.1038  0.9091 | 0.0938  0.9173  |  1.2 min 
 37.0     369    0.0010   |  0.0961  | 0.0938  0.9210 | 0.0938  0.9174  |  1.2 min 
 38.0     369    0.0010   |  0.0971  | 0.1105  0.9108 | 0.0931  0.9191  |  1.3 min 
 39.0     369    0.0010   |  0.0985  | 0.0952  0.9273 | 0.0933  0.9193  |  1.3 min 
 40.0     369    0.0010   |  0.0957  | 0.1116  0.9069 | 0.0934  0.9181  |  1.3 min 
 41.0     369    0.0001   |  0.0925  | 0.0888  0.9235 | 0.0928  0.9192  |  1.3 min 
 42.0     369    0.0001   |  0.0944  | 0.0953  0.9096 | 0.0927  0.9190  |  1.2 min 
 43.0     369    0.0001   |  0.0899  | 0.0960  0.9091 | 0.0931  0.9189  |  1.2 min 

/home/eugene/Documents/Kaggle_Planet/forest-developer/standard-eugene/results/dense121-jpg-0/snap/final.torch:
	all time to train=60.3 min
	test_loss=0.093110, test_acc=0.918895
